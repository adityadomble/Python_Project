# -*- coding: utf-8 -*-
"""Job Data.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zmaCFk_7_0hf-Sp0sxUILKtvfibP2lBa
"""

import numpy as np
import pandas as pd
import seaborn  as sns
import matplotlib.pyplot as plt

df=pd.read_csv("/content/jobs_dataset1222.csv")

df.head(5)

df.tail(5)

df.shape

df.columns.unique()

df = df.drop(columns=["url","jobType/1","jobType/2","jobType/3",])

df=df.drop(columns="externalApplyLink")

df.head(5)

df.shape

df.dtypes

df.info()

df.columns

df.describe()

#Univarriate Analysis

df.positionName.value_counts()

df.company.value_counts()

sns.histplot(data=df,x="rating",kde=True)
plt.xlabel('Rating')
plt.ylabel('Number of Jobs')
plt.title('histogram of Company rating')

"""# Insights

1.Most Companies are Highly Rated (3.5 to 4.5)

2.The peak (mode) is around   4.0, showing that most companies are rated favorably.

3.. Presence of Zero Ratings (Missing Data?)

A large spike at 0 Rating suggests many listings have missing or unfilled rating data.

4.Few Low-Rated Companies (< 3.0)

5.The KDE curve shows a right-skewed
"""

#Histogram of Salary

def extract_avg_salary(salary_str):
    if pd.isna(salary_str):
        return np.nan
    try:
        # Handle ranges like '$166,000 - $244,000 a year'
        if ' - ' in salary_str:
            salaries = [float(s.replace('$', '').replace(',', '').strip()) for s in salary_str.split(' - ')]
            return np.mean(salaries)
        # Handle hourly rates like '$70.67 an hour' - assume a 40-hour work week and 52 weeks a year
        elif 'an hour' in salary_str:
            hourly_rate = float(salary_str.replace('$', '').replace('an hour', '').replace(',', '').strip())
            return hourly_rate * 40 * 52
        # Handle single values like '$100,000 a year'
        elif 'a year' in salary_str:
             return float(salary_str.replace('$', '').replace('a year', '').replace(',', '').strip())
        else:
            return np.nan
    except:
        return np.nan

df['avg_salary']=df['salary'].apply(extract_avg_salary)

# Plot Histogram of Salary
plt.figure(figsize=(10,6))
plt.hist(df['avg_salary'].dropna(), bins=20, edgecolor='black', color='skyblue')
plt.title('Histogram of Salary Distribution')
plt.xlabel('Average Salary (USD/year)')
plt.ylabel('Number of Jobs')
plt.show()

"""Insights-
1. Salary distributon Is Right Skewed

2.There Are few Jobs Are Extremly High Paying(Also Outlier Is Present)

3.The most Common Salary package Around  $150,000 to $180,000(most job Salary Bracket)

4. there Are bar Near 0 To $150,000 (This Could be Internship,part-time jobs,   Incorrect data Entries)

5.Outlier Is Present ( Paying over $300,000 are rare)
"""

df.columns

unique_jobtypes = df['jobType/0'].unique()

unique_jobtypes

df['jobType/0'].value_counts(dropna=False)

#Bivariate Analysis

df.info()

#scatter plot of rating And salary

def extract_avg_salary(salary_str):
    if pd.isna(salary_str):
        return np.nan
    try:
        # Handle ranges like '$166,000 - $244,000 a year'
        if ' - ' in salary_str:
            salaries = [float(s.replace('$', '').replace(',', '').strip()) for s in salary_str.split(' - ')]
            return np.mean(salaries)
        # Handle hourly rates like '$70.67 an hour' - assume a 40-hour work week and 52 weeks a year
        elif 'an hour' in salary_str:
            hourly_rate = float(salary_str.replace('$', '').replace('an hour', '').replace(',', '').strip())
            return hourly_rate * 40 * 52
        # Handle single values like '$100,000 a year'
        elif 'a year' in salary_str:
             return float(salary_str.replace('$', '').replace('a year', '').replace(',', '').strip())
        else:
            return np.nan
    except:
        return np.nan

df['rating']=pd.to_numeric(df['rating'],errors='coerce')

df['salary_col'] = pd.to_numeric(df['avg_salary'], errors='coerce')

df_clean=df.dropna(subset=['rating','avg_salary'])

sns.scatterplot(x=df_clean["rating"], y=df_clean["avg_salary"])
plt.xticks()
plt.xlabel('Rating')
plt.ylabel('Average Salary ')
plt.title('Scatter Plot Of salary vs rating')

"""# Insights

1.No Strong Correlation Between Salary and Rating

2.The scatter plot shows that higher company ratings do not consistently with higher salaries.

3.Zero Ratings with High Salaries(Data entries with missing rating values)

4.Outliers: Very High Salaries (> $300,000)

5.No Clear Trendline Observed(Job role, location, and experience level may have a stronger impact)

"""

df.columns

# Countplot Of JobType/0
sns.countplot(data=df,x="jobType/0")
plt.title("No.of company offered Job Type")
plt.xlabel("Job Type")
plt.ylabel("No Of Companies")

"""#Insights

1.Full-time Positions Dominate the Market

2.This indicates that companies are primarily looking for long-term, permanent employees.

3.Contract Positions are the Second Most Common.

4.Minimal Part-time, Temporary, and Internship Offers

5.Hiring Strategy Insight Organizations prefer to hire full-time data          professionals

"""

position_count=df.company.value_counts()

position_count

df_sorted=df.sort_values(by='positionName')
print(df_sorted)

Top_3_companies=df_sorted.head(10)

Top_3_companies

top_10=df.sort_values(by="positionName",ascending=False).head(10)

top_10



# Count the number of job postings per company and get the top 10
top_companies = df['company'].value_counts().head(10)

# Plot the bar chart
plt.figure(figsize=(10, 6))
sns.barplot(x=top_companies.values, y=top_companies.index, palette='viridis')
plt.xlabel('Number of Job Postings')
plt.ylabel('Company')
plt.title('Top 10 Companies by Number of Job Postings')
plt.tight_layout()
plt.show()

"""#Insights

1.Amazon Leads in Job Postings

2.Both Amazon.com Services LLC and Amazon Web Services (AWS) also appear in the top10.

3.Big Tech Dominates the List
Other major tech giants like Apple, Meta (Facebook), and Google follow Amazon in job postings.

4. Presence of Financial Sector
Wells Fargo and JPMorgan Chase represent the banking & financial services sector, indicating that data-related roles.

5.Overall, the job market for data roles is highly concentrated among top-tier.
"""

#corr_matrix of Heatmap

corr_matrix=df[["rating","avg_salary"]].corr()

corr_matrix

sns.heatmap(corr_matrix,annot=True,cmap='coolwarm')
plt.title('correlation Heatmap of Numeric variables')

"""#Insights

1.The correlation coefficient between Rating and Avg Salary is 0.24.

2.This indicates a weak positive relationship     higher-rated companies tend to offer slightly higher salaries, but the correlation is not strong.

3.we know most high-paying jobs cluster around companies rated between 3.5 to 5, but exceptions exist

4.Conclusion from Correlation Analysis
Company Rating is not a reliable predictor of Salary in this dataset.
"""

#box[plot] of top 10 compamiers number of Jobs

top_companies = df['company'].value_counts().head(10)

top_companies

df['avg_salary'] = pd.to_numeric(df['avg_salary'], errors='coerce')

top_locations=df['location'].value_counts().nlargest(10).index

df_top_locations=df[df['location'].isin(top_locations)]

# Plot Boxplot of Salary by Location
plt.figure(figsize=(12, 6))
sns.boxplot(x='location', y='avg_salary', data=df_top_locations)
plt.title('Boxplot of Salary by Location')
plt.xlabel('Location')
plt.ylabel('Average Salary (USD/year)')
plt.xticks(rotation=45)
plt.show()

"""#Insights

1.The salary distribution across top locations shows that San Francisco offers the highest median pay, with significant outliers in high-paying positions.

2.Remote roles display a wide salary variation, highlighting opportunities across all experience levels.

3 Tech hubs like Sunnyvale and Washington, DC maintain competitive salaries with stable ranges

4.while cities like Austin and Charlotte reflect lower median salaries, likely due to a concentration of mid-level roles.
"""



df['avg_salary'] = df['salary'].apply(extract_avg_salary)

# Drop rows where average salary is NaN
df_clean = df.dropna(subset=['avg_salary'])

# Group by company and calculate average salary and job count
grouped_salary_by_company = df_clean.groupby('company').agg(
    avg_salary=('avg_salary', 'mean'),
    job_count=('avg_salary', 'count')
).sort_values(by='avg_salary', ascending=False)

# Display top 10 companies by average salary
print(grouped_salary_by_company.head(10))

df['avg_rating']=df['rating'].mean()

df['avg_rating']

avg_salary = df['avg_salary'].mean()
job_count = grouped_salary_by_company['job_count'].sum()



avg_rating_by_position=df.groupby('positionName')['rating'].mean().head(10)

salary_by_location=df.groupby('location')['salary'].head(10)

salary_by_location

df_clean=df.dropna(subset=['location','avg_salary'])

df_clean





#avg rating By position

avg_rating_by_position=avg_rating_by_position.sort_values(ascending=False)

avg_rating_by_position

#Count of jobs per country.

df['searchInput/country'].value_counts()

job_count=job_count.sum()

jobs_per_country=df.groupby('searchInput/country').size().reset_index(name='Job_per_country')
print('jobs_per_country')

jobs_per_country

#Group by location and compute max salary.

Avg_salary_by_location=df.groupby('location')['avg_salary'].mean().sort_values(ascending=False).head(5).reset_index(name='salary by location')

Avg_salary_by_location

#jobs per combination of location and job type.

jobs_per_combination=df.groupby(['location','jobType/0']).size().reset_index(name='job_count')

jobs_per_combination.head(10)

jobs_per_combination=jobs_per_combination.sort_values(by='job_count',ascending=False)

jobs_per_combination



"""ðŸ“— Filtering & Querying


"""

# jobs with salary > 15 LPA.

# jobs with salary > 15 LPA.
# Assuming 15 LPA is 1,500,000 USD
jobs_over_15lpa = df[df['avg_salary'] > 1500000]
print("Number of jobs with average salary > 15 LPA:")
print(jobs_over_15lpa.shape[0])

salary_over_15lpa=df[df['avg_salary'] > 1500000]

salary_over_15lpa

"""There is No jobs are available with salary more than 15 lpa"""

#Jobs where rating < 2.

rating_less_than2=df[df['rating']<2]

rating_less_than2

#Jobs posted in 'Mumbai' or 'Bangalore'.

# Jobs posted in 'Mumbai' or 'Bangalore'.

job_posted_in_Mi_Beng =df[df['location'].str.contains('Mumbai|Bangalore', na=False)]

job_posted_in_Mi_Beng

#Jobs with 'remote' in the description.

jobs_with_remote=df[df['description'].str.contains('remote', na=False)]

jobs_with_remote

#Jobs with title containing 'Engineer'.

df[df['positionName'].str.contains('Engineer',na=False)]

#ðŸ“— Text & Feature Engineering

#Word count in each description.



df['description_word_count'] = df['description'].str.split().str.len()
display(df[['description', 'description_word_count']].head())

#Most common words in job descriptions.

import re
from collections import Counter

Text="".join(df['description'].dropna()).lower()

Text

words=re.findall(r'\b[a-z]{2,}\b',Text)

common_word=Counter(words).most_common(1)

common_word

print('most Common Word:',common_word[0][0],"->",common_word[0][1],"times")

